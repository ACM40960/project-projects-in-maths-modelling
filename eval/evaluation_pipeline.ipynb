{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b74f2dc",
   "metadata": {},
   "source": [
    "# Pipeline Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the **full two-stage detection + classification pipeline** using ONNX-exported models for MegaDetector v6 and a ConvNeXt animal species classifier.  \n",
    "It tests performance across **cis** (in-domain) and **trans** (out-of-domain) validation and test sets.\n",
    "\n",
    "## Main Steps\n",
    "\n",
    "1. **Load ONNX Models**\n",
    "   - MegaDetector v6 (`megadetectorv6.onnx`) for detecting animals/vehicles.\n",
    "   - ConvNeXt classifier (`convnext_classifier.onnx`) for identifying 13 animal species.\n",
    "\n",
    "2. **Run MegaDetector on All Splits**\n",
    "   - Processes all images in `cis_val`, `cis_test`, `trans_val`, and `trans_test`.\n",
    "   - Saves raw detection predictions (`*_md_preds.csv`) with bounding boxes, confidence scores, and coarse class labels (animal/vehicle).\n",
    "\n",
    "3. **Classify Animal Detections**\n",
    "   - For each detection with class `\"animal\"`, crops the bounding box and runs the ConvNeXt classifier.\n",
    "   - Assigns the most probable species along with confidence score.\n",
    "   - Vehicles are kept as `\"car\"` without classification.\n",
    "   - Applies a **confidence threshold (0.55)** to filter low-confidence classifications.\n",
    "\n",
    "4. **Save Final Predictions**\n",
    "   - Outputs per-split final results as COCO-style JSON files (`*_final_predictions.json`).\n",
    "   - Logs confidence score statistics before/after thresholding for debugging.\n",
    "\n",
    "5. **Match Predictions to Ground Truth**\n",
    "   - Uses IoU (≥ 0.3) or containment check (≥ 60%) to match detections to ground-truth annotations.\n",
    "   - Records:\n",
    "     - Matched predictions\n",
    "     - Unmatched predictions\n",
    "     - Per-class and overall accuracies\n",
    "\n",
    "6. **Generate and Save Evaluation Reports**\n",
    "   - **Classification reports** (precision, recall, F1-score) per split.\n",
    "   - **Comprehensive metrics CSV** (overall accuracy, weighted F1, per-class accuracies).\n",
    "   - **Confusion matrices** (CSV + normalized PNG heatmaps).\n",
    "   - CSVs of unmatched predictions for error analysis.\n",
    "\n",
    "## Purpose\n",
    "This notebook provides an **end-to-end evaluation** of the combined detection + classification pipeline, showing both:\n",
    "- Detection quality from MegaDetector\n",
    "- Species classification performance from ConvNeXt  \n",
    "across in-domain and out-of-domain conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde66390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import onnxruntime as ort\n",
    "from pathlib import Path\n",
    "import json\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# --- Paths ---\n",
    "ROOT = Path(\"../\")\n",
    "SPLITS = [\"cis_val\", \"cis_test\", \"trans_val\", \"trans_test\"]\n",
    "IMG_DIRS = {split: ROOT/ \"data\" / \"megadetector_images\" / split / \"images\" for split in SPLITS }\n",
    "JSON_PATHS = {split: ROOT / \"annotations\" / f\"{split}.json\" for split in SPLITS}\n",
    "\n",
    "# ONNX models\n",
    "MD_ONNX_PATH = ROOT / \"models\" / \"megadetectorv6.onnx\"\n",
    "CLS_ONNX_PATH = ROOT / \"models\" / \"convnext_classifier.onnx\"\n",
    "\n",
    "# Output directory for detections and metrics\n",
    "OUTPUT_DIR = ROOT / \"eval\" / \"pipeline_results\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load ONNX models\n",
    "md_sess = ort.InferenceSession(str(MD_ONNX_PATH), providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "cls_sess = ort.InferenceSession(str(CLS_ONNX_PATH), providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"])\n",
    "\n",
    "print(\"Models loaded successfully.\")\n",
    "\n",
    "# Category mapping\n",
    "CATEGORY_NAME_TO_ID = {\n",
    "    \"bobcat\": 6,\n",
    "    \"opossum\": 1,\n",
    "    \"coyote\": 9,\n",
    "    \"raccoon\": 3,\n",
    "    \"bird\": 11,\n",
    "    \"dog\": 8,\n",
    "    \"cat\": 16,\n",
    "    \"squirrel\": 5,\n",
    "    \"rabbit\": 10,\n",
    "    \"skunk\": 7,\n",
    "    \"rodent\": 99,\n",
    "    \"badger\": 21,\n",
    "    \"deer\": 34,\n",
    "    \"car\": 33  # for detector only\n",
    "}\n",
    "CATEGORY_ID_TO_NAME = {v: k for k, v in CATEGORY_NAME_TO_ID.items()}\n",
    "\n",
    "# Classifier class list (excluding car)\n",
    "CLASSIFIER_CLASSES = [\n",
    "    \"badger\", \"bird\", \"bobcat\", \"cat\", \"coyote\", \"deer\", \"dog\", \"opossum\",\n",
    "    \"rabbit\", \"raccoon\", \"rodent\", \"skunk\", \"squirrel\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_megadetector_session(model_path):\n",
    "    return ort.InferenceSession(model_path)\n",
    "\n",
    "def preprocess_image(img, input_size=(640, 640)):\n",
    "    img_resized = cv2.resize(img, input_size)\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    img_norm = img_rgb.astype(np.float32) / 255.0\n",
    "    img_transposed = np.transpose(img_norm, (2, 0, 1))  # HWC - CHW\n",
    "    return img_transposed[np.newaxis, :, :, :]  \n",
    "\n",
    "def run_megadetector_on_split(split, session, conf_th=0.35, out_dir=Path(\"pipeline_results\")):\n",
    "    img_dir = IMG_DIRS[split]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_path = out_dir / f\"{split}_md_preds.csv\"\n",
    "    records = []\n",
    "\n",
    "    for img_path in tqdm(list(img_dir.glob(\"*.jpg\")), desc=f\"Running MD on {split}\"):\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            print(f\" Failed to read {img_path}\")\n",
    "            continue\n",
    "\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_resized = cv2.resize(img_rgb, (640, 640))\n",
    "        img_input = img_resized.astype(np.float32) / 255.0\n",
    "        img_input = np.transpose(img_input, (2, 0, 1))[np.newaxis, :, :, :]\n",
    "\n",
    "        outputs = session.run(None, {\"images\": img_input})\n",
    "        raw = outputs[0][0]  # shape: (N, 6)\n",
    "\n",
    "        scale_x = orig_w / 640\n",
    "        scale_y = orig_h / 640\n",
    "\n",
    "        for det in raw:\n",
    "            x1, y1, x2, y2, conf, cls_id = det\n",
    "            if conf < conf_th:\n",
    "                continue\n",
    "\n",
    "            cls_id = int(cls_id)\n",
    "            if cls_id == 0:\n",
    "                cls_name = \"animal\"\n",
    "            elif cls_id == 1 or cls_id == 2:\n",
    "                cls_name = \"vehicle\"\n",
    "            else:\n",
    "                continue  # skip unknowns\n",
    "\n",
    "            # Rescale to original size\n",
    "            x1 *= scale_x\n",
    "            y1 *= scale_y\n",
    "            x2 *= scale_x\n",
    "            y2 *= scale_y\n",
    "\n",
    "            records.append({\n",
    "                \"filename\": img_path.name,\n",
    "                \"x1\": float(x1), \"y1\": float(y1), \"x2\": float(x2), \"y2\": float(y2),\n",
    "                \"conf\": float(conf),\n",
    "                \"class\": cls_name,\n",
    "                \"class_id\": cls_id\n",
    "            })\n",
    "\n",
    "    pd.DataFrame(records).to_csv(csv_path, index=False)\n",
    "    print(f\" Saved predictions for {split} → {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7d9964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MD on cis_val: 100%|██████████| 1764/1764 [07:41<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions for cis_val → pipeline_results\\cis_val_md_preds.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MD on cis_test: 100%|██████████| 12141/12141 [55:41<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions for cis_test → pipeline_results\\cis_test_md_preds.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MD on trans_val: 100%|██████████| 1972/1972 [09:08<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions for trans_val → pipeline_results\\trans_val_md_preds.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running MD on trans_test: 100%|██████████| 18553/18553 [1:32:00<00:00,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions for trans_test → pipeline_results\\trans_test_md_preds.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "md_model_path = \"../models/megadetectorv6.onnx\" \n",
    "session = load_megadetector_session(md_model_path)\n",
    "\n",
    "for split in SPLITS:\n",
    "    run_megadetector_on_split(split, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26788079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_box(x1, y1, x2, y2, img_w, img_h):\n",
    "    x1 = max(0, min(int(x1), img_w - 1))\n",
    "    y1 = max(0, min(int(y1), img_h - 1))\n",
    "    x2 = max(0, min(int(x2), img_w - 1))\n",
    "    y2 = max(0, min(int(y2), img_h - 1))\n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca9ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing split: cis_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cis_val: 100%|██████████| 1645/1645 [00:19<00:00, 85.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cis_val - Confidence Analysis:\n",
      "Total detections before threshold: 1779\n",
      "Total detections after threshold (≥0.55): 1748\n",
      "Rejected: 31\n",
      "Confidence stats (before threshold):\n",
      "  Mean: 0.888\n",
      "  Median: 0.892\n",
      "  Min: 0.238\n",
      "  Max: 0.996\n",
      "  <0.3: 2\n",
      "  0.3-0.5: 18\n",
      "  0.5-0.55: 11\n",
      "  ≥0.55: 1748\n",
      "Saved 1748 predictions to ..\\eval\\pipeline_results\\cis_val_final_predictions.json\n",
      "\n",
      "Processing split: cis_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing cis_test: 100%|██████████| 11685/11685 [02:28<00:00, 78.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " cis_test - Confidence Analysis:\n",
      "Total detections before threshold: 12558\n",
      "Total detections after threshold (≥0.55): 12286\n",
      "Rejected: 272\n",
      "Confidence stats (before threshold):\n",
      "  Mean: 0.889\n",
      "  Median: 0.897\n",
      "  Min: 0.121\n",
      "  Max: 0.996\n",
      "  <0.3: 24\n",
      "  0.3-0.5: 156\n",
      "  0.5-0.55: 95\n",
      "  ≥0.55: 12283\n",
      "Saved 12286 predictions to ..\\eval\\pipeline_results\\cis_test_final_predictions.json\n",
      "\n",
      "Processing split: trans_val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trans_val: 100%|██████████| 1780/1780 [00:19<00:00, 92.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " trans_val - Confidence Analysis:\n",
      "Total detections before threshold: 1935\n",
      "Total detections after threshold (≥0.55): 1863\n",
      "Rejected: 72\n",
      "Confidence stats (before threshold):\n",
      "  Mean: 0.866\n",
      "  Median: 0.881\n",
      "  Min: 0.183\n",
      "  Max: 0.995\n",
      "  <0.3: 13\n",
      "  0.3-0.5: 38\n",
      "  0.5-0.55: 21\n",
      "  ≥0.55: 1863\n",
      "Saved 1863 predictions to ..\\eval\\pipeline_results\\trans_val_final_predictions.json\n",
      "\n",
      "Processing split: trans_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trans_test: 100%|██████████| 16145/16145 [03:13<00:00, 83.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " trans_test - Confidence Analysis:\n",
      "Total detections before threshold: 17542\n",
      "Total detections after threshold (≥0.55): 16765\n",
      "Rejected: 777\n",
      "Confidence stats (before threshold):\n",
      "  Mean: 0.859\n",
      "  Median: 0.879\n",
      "  Min: 0.119\n",
      "  Max: 0.996\n",
      "  <0.3: 144\n",
      "  0.3-0.5: 509\n",
      "  0.5-0.55: 141\n",
      "  ≥0.55: 16748\n",
      "Saved 16765 predictions to ..\\eval\\pipeline_results\\trans_test_final_predictions.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Config\n",
    "CONF_THRESH = 0.55\n",
    "IMG_SIZE = 224\n",
    "input_name = cls_sess.get_inputs()[0].name\n",
    "\n",
    "def preprocess_crop(img):\n",
    "   \n",
    "    # Convert BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Direct resize to 224x224\n",
    "    resized = cv2.resize(img_rgb, (224, 224))\n",
    "    \n",
    "    # Normalize\n",
    "    norm = resized.astype(np.float32) / 255.0\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    norm = (norm - mean) / std\n",
    "    \n",
    "    # Convert to CHW format and add batch dimension\n",
    "    transposed = np.transpose(norm, (2, 0, 1))\n",
    "    return transposed[np.newaxis, :, :, :]\n",
    "\n",
    "def debug_confidence_scores(split, results_before_threshold, results_after_threshold):\n",
    "    \"\"\"\n",
    "    Debug confidence score distributions\n",
    "    \"\"\"\n",
    "    # Extract confidence scores before and after thresholding\n",
    "    confs_before = [r[\"conf\"] for r in results_before_threshold if r.get(\"conf\")]\n",
    "    confs_after = [r[\"conf\"] for r in results_after_threshold if r.get(\"conf\")]\n",
    "    \n",
    "    print(f\"\\n {split} - Confidence Analysis:\")\n",
    "    print(f\"Total detections before threshold: {len(confs_before)}\")\n",
    "    print(f\"Total detections after threshold (≥{CONF_THRESH}): {len(confs_after)}\")\n",
    "    print(f\"Rejected: {len(confs_before) - len(confs_after)}\")\n",
    "    \n",
    "    if confs_before:\n",
    "        print(f\"Confidence stats (before threshold):\")\n",
    "        print(f\"  Mean: {np.mean(confs_before):.3f}\")\n",
    "        print(f\"  Median: {np.median(confs_before):.3f}\")\n",
    "        print(f\"  Min: {np.min(confs_before):.3f}\")\n",
    "        print(f\"  Max: {np.max(confs_before):.3f}\")\n",
    "        print(f\"  <0.3: {sum(c < 0.3 for c in confs_before)}\")\n",
    "        print(f\"  0.3-0.5: {sum(0.3 <= c < 0.5 for c in confs_before)}\")\n",
    "        print(f\"  0.5-0.55: {sum(0.5 <= c < 0.55 for c in confs_before)}\")\n",
    "        print(f\"  ≥0.55: {sum(c >= 0.55 for c in confs_before)}\")\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # numerical stability\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# Process each split\n",
    "for split in SPLITS:\n",
    "    print(f\"\\nProcessing split: {split}\")\n",
    "    md_csv = OUTPUT_DIR / f\"{split}_md_preds.csv\"\n",
    "    df = pd.read_csv(md_csv)\n",
    "\n",
    "    results_all = []  # Before thresholding\n",
    "    results = []      # After thresholding\n",
    "    img_dir = IMG_DIRS[split]\n",
    "    grouped = df.groupby(\"filename\")\n",
    "\n",
    "    for filename, rows in tqdm(grouped, desc=f\"Processing {split}\"):\n",
    "        img_path = img_dir / filename\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "        H, W = img.shape[:2]\n",
    "\n",
    "        for _, row in rows.iterrows():\n",
    "            x1, y1, x2, y2 = clip_box(row.x1, row.y1, row.x2, row.y2, W, H)\n",
    "\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "\n",
    "            if row[\"class\"] == \"vehicle\":\n",
    "                pred = {\n",
    "                    \"filename\": filename,\n",
    "                    \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
    "                    \"category\": \"car\",\n",
    "                    \"category_id\": 33,\n",
    "                    \"conf\": float(row.conf)\n",
    "                }\n",
    "                results_all.append(pred)\n",
    "                results.append(pred)  # No threshold for vehicles\n",
    "                \n",
    "            elif row[\"class\"] == \"animal\":\n",
    "                crop = img[y1:y2, x1:x2]\n",
    "                if crop.size == 0:\n",
    "                    continue\n",
    "\n",
    "                inp = preprocess_crop(crop)  # Use the fixed preprocessing\n",
    "                logits = cls_sess.run(None, {input_name: inp})[0][0]\n",
    "                \n",
    "                probs = softmax(logits.astype(np.float32))\n",
    "                cls_idx = np.argmax(probs)\n",
    "                conf = probs[cls_idx]\n",
    "                \n",
    "                name = CLASSIFIER_CLASSES[cls_idx]\n",
    "                coco_id = CATEGORY_NAME_TO_ID[name]\n",
    "\n",
    "                pred = {\n",
    "                    \"filename\": filename,\n",
    "                    \"bbox\": [int(x1), int(y1), int(x2), int(y2)],\n",
    "                    \"category\": str(name),\n",
    "                    \"category_id\": int(coco_id),\n",
    "                    \"conf\": float(conf)\n",
    "                }\n",
    "                results_all.append(pred)\n",
    "                \n",
    "                # Only add to final results if above threshold\n",
    "                if conf >= CONF_THRESH:\n",
    "                    results.append(pred)\n",
    "                \n",
    "    debug_confidence_scores(split, results_all, results)\n",
    "\n",
    "    # Save final predictions\n",
    "    out_json = OUTPUT_DIR / f\"{split}_final_predictions.json\"\n",
    "    with open(out_json, \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"Saved {len(results)} predictions to {out_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50955248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating split: cis_val\n",
      "  Total predictions: 1748\n",
      "  Matched (IoU ≥ 0.3): 1702\n",
      "  Unmatched: 46\n",
      "  Overall Accuracy: 0.981\n",
      "  Weighted F1-Score: 0.981\n",
      "  Per-class Accuracy:\n",
      "    bird: 0.957\n",
      "    bobcat: 0.952\n",
      "    car: 1.000\n",
      "    cat: 0.975\n",
      "    coyote: 0.978\n",
      "    deer: 1.000\n",
      "    dog: 0.980\n",
      "    opossum: 0.989\n",
      "    rabbit: 0.992\n",
      "    raccoon: 0.976\n",
      "    rodent: 1.000\n",
      "    skunk: 1.000\n",
      "    squirrel: 0.966\n",
      "\n",
      "Evaluating split: trans_val\n",
      "  Total predictions: 1863\n",
      "  Matched (IoU ≥ 0.3): 1783\n",
      "  Unmatched: 80\n",
      "  Overall Accuracy: 0.942\n",
      "  Weighted F1-Score: 0.946\n",
      "  Per-class Accuracy:\n",
      "    bird: 0.842\n",
      "    bobcat: 0.934\n",
      "    car: 1.000\n",
      "    cat: 0.787\n",
      "    coyote: 0.967\n",
      "    dog: 0.938\n",
      "    opossum: 0.968\n",
      "    rabbit: 1.000\n",
      "    raccoon: 0.907\n",
      "    rodent: 0.400\n",
      "    skunk: 0.950\n",
      "    squirrel: 0.935\n",
      "\n",
      "Evaluating split: cis_test\n",
      "  Total predictions: 12286\n",
      "  Matched (IoU ≥ 0.3): 11948\n",
      "  Unmatched: 338\n",
      "  Overall Accuracy: 0.977\n",
      "  Weighted F1-Score: 0.977\n",
      "  Per-class Accuracy:\n",
      "    badger: 0.000\n",
      "    bird: 0.987\n",
      "    bobcat: 0.977\n",
      "    car: 1.000\n",
      "    cat: 0.965\n",
      "    coyote: 0.972\n",
      "    deer: 0.919\n",
      "    dog: 0.982\n",
      "    opossum: 0.979\n",
      "    rabbit: 0.985\n",
      "    raccoon: 0.975\n",
      "    rodent: 0.968\n",
      "    skunk: 0.980\n",
      "    squirrel: 0.949\n",
      "\n",
      "Evaluating split: trans_test\n",
      "  Total predictions: 16765\n",
      "  Matched (IoU ≥ 0.3): 16049\n",
      "  Unmatched: 716\n",
      "  Overall Accuracy: 0.937\n",
      "  Weighted F1-Score: 0.938\n",
      "  Per-class Accuracy:\n",
      "    badger: 0.000\n",
      "    bird: 0.912\n",
      "    bobcat: 0.914\n",
      "    car: 1.000\n",
      "    cat: 0.788\n",
      "    coyote: 0.949\n",
      "    dog: 0.951\n",
      "    opossum: 0.978\n",
      "    rabbit: 0.895\n",
      "    raccoon: 0.926\n",
      "    rodent: 0.375\n",
      "    skunk: 0.988\n",
      "    squirrel: 0.940\n",
      "\n",
      "Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "GT_FILES = {\n",
    "    \"cis_val\": \"cis_val_fixed.json\",\n",
    "    \"trans_val\": \"trans_val_fixed.json\",\n",
    "    \"cis_test\": \"cis_test_noverify.json\",\n",
    "    \"trans_test\": \"trans_test_noverify.json\"\n",
    "}\n",
    "\n",
    "GT_DIR = Path(\"../data/preprocessed/annotations/cleaned\")\n",
    "IOU_THRESHOLD = 0.3\n",
    "OUTPUT_CSV_DIR = Path(\"pipeline_results/eval_reports\")\n",
    "OUTPUT_CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def gt_in_pred(gt_box, pred_box, containment_thresh=0.9):\n",
    "    \"\"\"\n",
    "    Returns True if the ground-truth box is at least 90% contained within the predicted box.\n",
    "    \"\"\"\n",
    "    gx1, gy1, gx2, gy2 = gt_box\n",
    "    px1, py1, px2, py2 = pred_box\n",
    "\n",
    "    # Compute intersection rectangle\n",
    "    ix1 = max(gx1, px1)\n",
    "    iy1 = max(gy1, py1)\n",
    "    ix2 = min(gx2, px2)\n",
    "    iy2 = min(gy2, py2)\n",
    "\n",
    "    inter_w = max(0, ix2 - ix1)\n",
    "    inter_h = max(0, iy2 - iy1)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    gt_area = max(1, (gx2 - gx1) * (gy2 - gy1))\n",
    "\n",
    "    return (inter_area / gt_area) >= containment_thresh\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    \"\"\"\n",
    "    Compute Intersection over Union (IoU) between two bounding boxes.\n",
    "    Boxes are in [x1, y1, x2, y2] format.\n",
    "    \"\"\"\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = max(1, (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = max(1, (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "\n",
    "\n",
    "SPLITS = [\"cis_val\", \"trans_val\", \"cis_test\", \"trans_test\"]\n",
    "\n",
    "for split in SPLITS:\n",
    "    print(f\"\\nEvaluating split: {split}\")\n",
    "    \n",
    "    pred_file = f\"pipeline_results/{split}_final_predictions.json\"\n",
    "    gt_file = GT_DIR / GT_FILES[split]\n",
    "\n",
    "    preds = json.load(open(pred_file))\n",
    "    gt_data = json.load(open(gt_file))\n",
    "\n",
    "    image_id_to_name = {img[\"id\"]: img[\"file_name\"] for img in gt_data[\"images\"]}\n",
    "    gt_by_filename = defaultdict(list)\n",
    "    for ann in gt_data[\"annotations\"]:\n",
    "        fn = image_id_to_name[ann[\"image_id\"]]\n",
    "        x, y, w, h = ann[\"bbox\"]\n",
    "        gt_box = [x, y, x + w, y + h]\n",
    "        gt_by_filename[fn].append({\"bbox\": gt_box, \"category_id\": ann[\"category_id\"]})\n",
    "    id_to_name = {cat[\"id\"]: cat[\"name\"] for cat in gt_data[\"categories\"]}\n",
    "\n",
    "    y_true, y_pred, unmatched_preds = [], [], []\n",
    "    \n",
    "    for pred in preds:\n",
    "        fn = pred[\"filename\"]\n",
    "        pred_box = pred[\"bbox\"]\n",
    "        pred_cat = pred[\"category_id\"]\n",
    "\n",
    "        best_gt = None\n",
    "        for gt in gt_by_filename.get(fn, []):\n",
    "            gt_box = gt[\"bbox\"]\n",
    "            iou_score = iou(pred_box, gt_box)\n",
    "            containment = gt_in_pred(gt_box, pred_box, containment_thresh=0.6) \n",
    "            \n",
    "            # Simple OR condition - accept if EITHER condition is met\n",
    "            if iou_score >= IOU_THRESHOLD or containment:\n",
    "                best_gt = gt\n",
    "                break  # Take the first match that satisfies either condition\n",
    "\n",
    "        if best_gt:\n",
    "            y_true.append(best_gt[\"category_id\"])\n",
    "            y_pred.append(pred_cat)\n",
    "        else:\n",
    "            unmatched_preds.append(pred)\n",
    "\n",
    "    print(f\"  Total predictions: {len(preds)}\")\n",
    "    print(f\"  Matched (IoU ≥ {IOU_THRESHOLD}): {len(y_true)}\")\n",
    "    print(f\"  Unmatched: {len(unmatched_preds)}\")\n",
    "\n",
    "    # Calculate accuracy metrics\n",
    "    y_true_names = [id_to_name.get(i, str(i)) for i in y_true]\n",
    "    y_pred_names = [id_to_name.get(i, str(i)) for i in y_pred]\n",
    "    \n",
    "    # Overall accuracy\n",
    "    overall_accuracy = accuracy_score(y_true_names, y_pred_names)\n",
    "    print(f\"  Overall Accuracy: {overall_accuracy:.3f}\")\n",
    "\n",
    "    # Classification report for detailed metrics\n",
    "    report_dict = classification_report(\n",
    "        y_true_names,\n",
    "        y_pred_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Weighted accuracy\n",
    "    weighted_accuracy = report_dict['weighted avg']['f1-score'] \n",
    "    print(f\"  Weighted F1-Score: {weighted_accuracy:.3f}\")\n",
    "\n",
    "    # Per-class accuracy\n",
    "    print(\"  Per-class Accuracy:\")\n",
    "    class_accuracies = {}\n",
    "    for class_name in sorted(set(y_true_names)):\n",
    "        # Get indices for this class in true labels\n",
    "        class_indices = [i for i, true_label in enumerate(y_true_names) if true_label == class_name]\n",
    "        if class_indices:\n",
    "            class_true = [y_true_names[i] for i in class_indices]\n",
    "            class_pred = [y_pred_names[i] for i in class_indices]\n",
    "            class_acc = accuracy_score(class_true, class_pred)\n",
    "            class_accuracies[class_name] = class_acc\n",
    "            print(f\"    {class_name}: {class_acc:.3f}\")\n",
    "\n",
    "    # Create comprehensive metrics summary\n",
    "    metrics_summary = {\n",
    "        \"split\": split,\n",
    "        \"total_predictions\": len(preds),\n",
    "        \"matched_predictions\": len(y_true),\n",
    "        \"unmatched_predictions\": len(unmatched_preds),\n",
    "        \"matching_rate\": len(y_true) / len(preds),\n",
    "        \"overall_accuracy\": overall_accuracy,\n",
    "        \"weighted_f1\": weighted_accuracy,\n",
    "        **{f\"{class_name}_accuracy\": acc for class_name, acc in class_accuracies.items()}\n",
    "    }\n",
    "    \n",
    "    # Save comprehensive metrics\n",
    "    metrics_df = pd.DataFrame([metrics_summary])\n",
    "    metrics_df.to_csv(OUTPUT_CSV_DIR / f\"{split}_comprehensive_metrics.csv\", index=False)\n",
    "\n",
    "    # Original outputs\n",
    "    report_df = pd.DataFrame(report_dict).transpose()\n",
    "    report_df.to_csv(OUTPUT_CSV_DIR / f\"{split}_classification_report.csv\")\n",
    "\n",
    "    cm = confusion_matrix(y_true_names, y_pred_names, labels=sorted(set(id_to_name.values())))\n",
    "    cm_df = pd.DataFrame(cm, index=sorted(set(id_to_name.values())), columns=sorted(set(id_to_name.values())))\n",
    "    cm_df.to_csv(OUTPUT_CSV_DIR / f\"{split}_confusion_matrix.csv\")\n",
    "\n",
    "    labels_sorted = sorted(set(id_to_name.values()))\n",
    "\n",
    "    # Normalize by true label (row-wise) - fix division by zero\n",
    "    cm_normalized = cm.astype(np.float64)\n",
    "    row_sums = cm_normalized.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "    cm_normalized = cm_normalized / row_sums\n",
    "    cm_normalized_df = pd.DataFrame(cm_normalized, index=labels_sorted, columns=labels_sorted)\n",
    "\n",
    "    # Save normalized confusion matrix as PNG\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized_df, annot=True, fmt=\".2f\", cmap=\"Blues\", vmin=0, vmax=1,\n",
    "                xticklabels=True, yticklabels=True)\n",
    "    plt.title(f\"{split} Normalized Confusion Matrix (Row-wise)\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_CSV_DIR / f\"{split}_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    pd.DataFrame(unmatched_preds).to_csv(OUTPUT_CSV_DIR / f\"{split}_unmatched_preds.csv\", index=False)\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-wildlife",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
